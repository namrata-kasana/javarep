from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark session (if not already available)
spark = SparkSession.builder.appName("DatabricksSQL").getOrCreate()

# Run 4 SQL queries and store results in separate DataFrames
df1 = spark.sql("SELECT COUNT(*) AS total_orders FROM orders")  # Example Query 1
df2 = spark.sql("SELECT AVG(price) AS avg_price FROM products")  # Example Query 2
df3 = spark.sql("SELECT MAX(sales) AS max_sales FROM sales")      # Example Query 3
df4 = spark.sql("SELECT MIN(discount) AS min_discount FROM discounts")  # Example Query 4

# Add a row ID column for sequential merging
df1 = df1.withColumn("row_id", col("total_orders") * 0 + 1)  
df2 = df2.withColumn("row_id", col("avg_price") * 0 + 1)  
df3 = df3.withColumn("row_id", col("max_sales") * 0 + 1)  
df4 = df4.withColumn("row_id", col("min_discount") * 0 + 1)  

# Merge DataFrames sequentially using row_id
final_df = df1.join(df2, "row_id", "outer") \
              .join(df3, "row_id", "outer") \
              .join(df4, "row_id", "outer") \
              .drop("row_id")  # Drop helper column

# Show final result
final_df.show()

# Store result in a Delta table
final_df.write.mode("overwrite").format("delta").saveAsTable("final_results")


from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Initialize Spark session (Databricks automatically provides one)
spark = SparkSession.builder.appName("SequentialDFs").getOrCreate()

# Define 4 DataFrames (Example Queries)
df1 = spark.sql("SELECT COUNT(*) AS total_orders FROM orders").withColumn("step", lit("Step 1"))
df2 = spark.sql("SELECT AVG(price) AS avg_price FROM products").withColumn("step", lit("Step 2"))
df3 = spark.sql("SELECT MAX(sales) AS max_sales FROM sales").withColumn("step", lit("Step 3"))
df4 = spark.sql("SELECT MIN(discount) AS min_discount FROM discounts").withColumn("step", lit("Step 4"))

# Run DataFrames sequentially
df1.show()
df2.show()
df3.show()
df4.show()

# Append all results in sequence
final_df = df1.unionByName(df2, allowMissingColumns=True) \
              .unionByName(df3, allowMissingColumns=True) \
              .unionByName(df4, allowMissingColumns=True)

# Show final result
final_df.show()

# Store in a Delta table
final_df.write.mode("overwrite").format("delta").saveAsTable("final_results")

